{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"competition","sourceId":24800,"datasetId":1042002,"databundleVersionId":1831594},{"sourceType":"datasetVersion","sourceId":13224382,"datasetId":8382328,"databundleVersionId":13919222},{"sourceType":"datasetVersion","sourceId":13279354,"datasetId":8415723,"databundleVersionId":13979328}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Run this single shell cell\n!pip install -q ultralytics==8.0.114 grad-cam pydicom timm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:48:04.873598Z","iopub.execute_input":"2025-10-06T14:48:04.874380Z","iopub.status.idle":"2025-10-06T14:49:33.748013Z","shell.execute_reply.started":"2025-10-06T14:48:04.874344Z","shell.execute_reply":"2025-10-06T14:49:33.747356Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m595.4/595.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n\nimport os, random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nimport pydicom\n\n# =====================\n# CONFIG\n# =====================\nOUT_IMG_SIZE = (512, 512)   # None -> keep original resolution\nMAX_IMAGES_PER_SPLIT = None  # set to an int for debugging\nRANDOM_SEED = 42\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n\n# Paths (update these according to Kaggle dataset mount)\nTRAIN_CSV = Path(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/train.csv\")\nSAMPLE_SUB = Path(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/sample_submission.csv\")\nTRAIN_DICOM_DIR = Path(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/train\")\nTEST_DICOM_DIR = Path(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/test\")\n\n\n# Create folders\nfor split in [\"train\", \"val\", \"test\"]:\n    (IMG_DIR / split).mkdir(parents=True, exist_ok=True)\n    (LAB_DIR / split).mkdir(parents=True, exist_ok=True)\n\n# =====================\n# Load train.csv\n# =====================\ndf = pd.read_csv(TRAIN_CSV)\nprint(\"Total annotation rows:\", len(df))\nunique_train_image_ids = df['image_id'].unique().tolist()\nprint(\"Unique images with annotations:\", len(unique_train_image_ids))\n\n# Collect all dicom files in train dir\nall_train_files = [f.stem for f in TRAIN_DICOM_DIR.iterdir() if f.suffix.lower() == '.dicom']\nimage_ids = sorted(list(set(all_train_files)))\nprint(\"Available dicom images in train folder:\", len(image_ids))\n\n# Train/val split (90/10)\nrandom.shuffle(image_ids)\nval_frac = 0.1\nn_val = int(len(image_ids) * val_frac)\nval_ids = image_ids[:n_val]\ntrain_ids = image_ids[n_val:]\nprint(\"Train ids:\", len(train_ids), \"Val ids:\", len(val_ids))\n\n# =====================\n# Conversion helpers\n# =====================\ndef dicom_to_pil(dicom_path):\n    ds = pydicom.dcmread(str(dicom_path))\n    img = ds.pixel_array.astype(np.float32)\n    lo, hi = np.percentile(img, (0.5, 99.5))\n    img = np.clip(img, lo, hi)\n    img = img - img.min()\n    if img.max() > 0:\n        img = img / img.max()\n    img = (img * 255).astype(np.uint8)\n    pil = Image.fromarray(img).convert(\"RGB\")\n    return pil\n\ndef write_yolo_label(image_id, boxes_for_image, out_label_path, img_w, img_h):\n    lines = []\n    for box in boxes_for_image:\n        x_min, y_min, x_max, y_max = box['x_min'], box['y_min'], box['x_max'], box['y_max']\n        class_id = int(box['class_id'])\n        x_min = max(0, x_min); y_min = max(0, y_min)\n        x_max = min(img_w-1, x_max); y_max = min(img_h-1, y_max)\n        width = x_max - x_min\n        height = y_max - y_min\n        if width <= 0 or height <= 0: \n            continue\n        cx = x_min + width/2.0\n        cy = y_min + height/2.0\n        cx_norm = cx / img_w\n        cy_norm = cy / img_h\n        w_norm = width / img_w\n        h_norm = height / img_h\n        lines.append(f\"{class_id} {cx_norm:.6f} {cy_norm:.6f} {w_norm:.6f} {h_norm:.6f}\")\n    if len(lines) == 0:\n        open(out_label_path, 'w').close()\n        return\n    with open(out_label_path, 'w') as f:\n        f.write(\"\\n\".join(lines))\n\n# =====================\n# Converters with SKIP logic\n# =====================\ndef convert_split(ids_list, split_name, max_images=None):\n    pbar = ids_list if max_images is None else ids_list[:max_images]\n    count, skipped = 0, 0\n    for img_id in pbar:\n        out_img_path = IMG_DIR / split_name / f\"{img_id}.jpg\"\n        out_lbl_path = LAB_DIR / split_name / f\"{img_id}.txt\"\n\n        # ✅ Skip if already exists\n        if out_img_path.exists() and out_lbl_path.exists():\n            skipped += 1\n            continue\n\n        dicom_path = TRAIN_DICOM_DIR / f\"{img_id}.dicom\"\n        if not dicom_path.exists():\n            continue\n        try:\n            pil = dicom_to_pil(dicom_path)\n        except Exception as e:\n            print(\"Failed to read\", dicom_path, e)\n            continue\n\n        if OUT_IMG_SIZE is not None:\n            pil = pil.resize(OUT_IMG_SIZE)\n\n        out_img_path.parent.mkdir(parents=True, exist_ok=True)\n        pil.save(out_img_path, quality=95)\n\n        boxes = df[df['image_id'] == img_id]\n        write_yolo_label(img_id, boxes.to_dict('records'), out_lbl_path, pil.width, pil.height)\n\n        count += 1\n        if count % 500 == 0:\n            print(f\"{split_name}: converted {count} new images (skipped {skipped})...\")\n    print(f\"Finished {split_name} -> converted {count}, skipped {skipped}.\")\n\ndef convert_test(ids_list, max_images=None):\n    pbar = ids_list if max_images is None else ids_list[:max_images]\n    count, skipped = 0, 0\n    for img_id in pbar:\n        out_img_path = IMG_DIR / \"test\" / f\"{img_id}.jpg\"\n        out_lbl_path = LAB_DIR / \"test\" / f\"{img_id}.txt\"\n\n        # ✅ Skip if already exists\n        if out_img_path.exists() and out_lbl_path.exists():\n            skipped += 1\n            continue\n\n        dicom_path = TEST_DICOM_DIR / f\"{img_id}.dicom\"\n        if not dicom_path.exists():\n            continue\n        try:\n            pil = dicom_to_pil(dicom_path)\n        except Exception as e:\n            print(\"Failed to read\", dicom_path, e)\n            continue\n\n        if OUT_IMG_SIZE is not None:\n            pil = pil.resize(OUT_IMG_SIZE)\n\n        out_img_path.parent.mkdir(parents=True, exist_ok=True)\n        pil.save(out_img_path, quality=95)\n\n        # empty label\n        open(out_lbl_path, 'w').close()\n\n        count += 1\n        if count % 500 == 0:\n            print(f\"test: converted {count} new images (skipped {skipped})...\")\n    print(f\"Finished test -> converted {count}, skipped {skipped}.\")\n\n# =====================\n# Run conversions\n# =====================\nprint(\"Converting TRAIN split ...\")\nconvert_split(train_ids, \"train\", max_images=MAX_IMAGES_PER_SPLIT)\n\nprint(\"Converting VAL split ...\")\nconvert_split(val_ids, \"val\", max_images=MAX_IMAGES_PER_SPLIT)\n\nprint(\"Converting TEST split ...\")\nsample_sub = pd.read_csv(SAMPLE_SUB)\ntest_ids = sample_sub['image_id'].tolist()\nconvert_test(test_ids, max_images=MAX_IMAGES_PER_SPLIT)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Python cell\nfrom pathlib import Path\nimport os, random, time, json, math\nimport numpy as np, pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport torchvision.transforms.functional as TF\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n\n# ultralytics\nfrom ultralytics import YOLO\n\n# global paths (update if needed)\nWORK_DIR = Path(\"/kaggle/working/vindr\")\nIMG_DIR = WORK_DIR / \"images\"\nLAB_DIR = WORK_DIR / \"labels\"\nMODEL_DIR = Path(\"/kaggle/working/models\")\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_CSV = Path(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/train.csv\")\nSAMPLE_SUB = Path(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/sample_submission.csv\")\nTEST_DICOM_DIR = Path(\"/kaggle/input/vinbigdata-chest-xray-abnormalities-detection/test\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:57:03.506020Z","iopub.execute_input":"2025-10-06T14:57:03.506288Z","iopub.status.idle":"2025-10-06T14:57:16.050839Z","shell.execute_reply.started":"2025-10-06T14:57:03.506267Z","shell.execute_reply":"2025-10-06T14:57:16.049912Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Build class list (match earlier)\nclass_names = [\n    \"Aortic_enlargement\",\"Atelectasis\",\"Calcification\",\"Cardiomegaly\",\n    \"Consolidation\",\"ILD\",\"Infiltration\",\"Lung_Opacity\",\"Nodule_Mass\",\n    \"Other_lesion\",\"Pleural_effusion\",\"Pleural_thickening\",\n    \"Pneumothorax\",\"Pulmonary_fibrosis\",\"No_finding\"\n]\nNUM_CLASSES = len(class_names)\n\n# Read train.csv and build multi-hot label dict\ndf = pd.read_csv(TRAIN_CSV)\n# group by image_id\ntargets = {}\nfor img_id, g in df.groupby(\"image_id\"):\n    vec = np.zeros(NUM_CLASSES, dtype=np.float32)\n    for cid in g['class_id'].values:\n        vec[int(cid)] = 1.0\n    targets[img_id] = vec\n\n# Some images may be missing in df (no annotation) -> treat as No_finding\n# Ensure that for converted images, we have label vectors\ntrain_img_dir = IMG_DIR / \"train\"\nval_img_dir   = IMG_DIR / \"val\"\n\ntrain_ids = [p.stem for p in train_img_dir.glob(\"*.jpg\")]\nval_ids   = [p.stem for p in val_img_dir.glob(\"*.jpg\")]\n\n# if an image not in targets -> treat as no finding (class 14 = No_finding)\nfor img in train_ids + val_ids:\n    if img not in targets:\n        vec = np.zeros(NUM_CLASSES, dtype=np.float32)\n        vec[14] = 1.0\n        targets[img] = vec\n\n# Dataset class\nclass MultiLabelCXRDataset(Dataset):\n    def __init__(self, image_dir, img_ids, targets_dict, transform=None):\n        self.image_dir = Path(image_dir)\n        self.img_ids = img_ids\n        self.targets = targets_dict\n        self.transform = transform\n\n    def __len__(self): return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        img_path = self.image_dir / f\"{img_id}.jpg\"\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        label = torch.tensor(self.targets[img_id], dtype=torch.float32)\n        return img, label, img_id\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:57:23.715605Z","iopub.execute_input":"2025-10-06T14:57:23.716166Z","iopub.status.idle":"2025-10-06T14:57:24.452126Z","shell.execute_reply.started":"2025-10-06T14:57:23.716068Z","shell.execute_reply":"2025-10-06T14:57:24.451362Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from torchvision import transforms\n\nBATCH = 32\n\ntrain_tfms = transforms.Compose([\n    # Randomly crop & resize while keeping aspect ratio\n    transforms.RandomResizedCrop(224, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n    \n    # Geometric augmentations\n    transforms.RandomHorizontalFlip(p=0.5),       # Chest X-rays: left/right flip is valid\n    transforms.RandomRotation(degrees=7),         # Small rotations, <= 7°\n    transforms.RandomAffine(\n        degrees=0, \n        translate=(0.02, 0.02), \n        scale=(0.95, 1.05), \n        shear=2\n    ),\n\n    # Photometric augmentations (mild)\n    transforms.ColorJitter(brightness=0.15, contrast=0.15),\n    transforms.RandomGrayscale(p=0.1),\n    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n\n    # Convert & normalize\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\nval_tfms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n])\n\n\ntrain_ds = MultiLabelCXRDataset(train_img_dir, train_ids, targets, transform=train_tfms)\nval_ds   = MultiLabelCXRDataset(val_img_dir, val_ids, targets, transform=val_tfms)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=4, pin_memory=True)\nval_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\nprint(\"Train / Val sizes:\", len(train_ds), len(val_ds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:57:27.095919Z","iopub.execute_input":"2025-10-06T14:57:27.096438Z","iopub.status.idle":"2025-10-06T14:57:27.104895Z","shell.execute_reply.started":"2025-10-06T14:57:27.096412Z","shell.execute_reply":"2025-10-06T14:57:27.104178Z"}},"outputs":[{"name":"stdout","text":"Train / Val sizes: 13500 1500\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet50, ResNet50_Weights\nfrom torch.amp import autocast, GradScaler\n\n# Model\nNUM_CLASSES = 15  # change as needed\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_cls = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nmodel_cls.fc = nn.Linear(model_cls.fc.in_features, NUM_CLASSES)\nmodel_cls = model_cls.to(device)\n\n# Loss, optimizer, scheduler\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.AdamW(model_cls.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\n\nscaler = GradScaler()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:57:34.682588Z","iopub.execute_input":"2025-10-06T14:57:34.683251Z","iopub.status.idle":"2025-10-06T14:57:36.103100Z","shell.execute_reply.started":"2025-10-06T14:57:34.683224Z","shell.execute_reply":"2025-10-06T14:57:36.102539Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 198MB/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport numpy as np\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n\n\nbest_val_loss = float(\"inf\")\nbest_path = MODEL_DIR / \"resnet50_multilabel_best.pth\"\n# =========================\n# Hyperparameters / Config\n# =========================\nBATCH = 32\nNUM_CLASSES = 15\nEPOCHS = 20            # <-- define it here, before the loop\nLEARNING_RATE = 1e-4\nWEIGHT_DECAY = 1e-5\n\nfor epoch in range(1, EPOCHS + 1):\n    print(f\"\\n===== Epoch {epoch}/{EPOCHS} =====\")\n    # ---------------- TRAIN ----------------\n    model_cls.train()\n    running_loss = 0.0\n    n_samples = 0\n\n    train_preds_cpu = []\n    train_targets_cpu = []\n\n    pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch}\", leave=False)\n    for batch in pbar:\n        # dataset returns (img, label) or (img, label, id)\n        if len(batch) == 3:\n            imgs, labels, _ = batch\n        else:\n            imgs, labels = batch\n\n        imgs = imgs.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True).float()\n\n        optimizer.zero_grad()\n\n        with autocast(device_type=\"cuda\"):   # correct autocast usage\n            outputs = model_cls(imgs)         # logits (B, C)\n            loss = criterion(outputs, labels)\n\n        # backward + step with scaler\n        scaler.scale(loss).backward()\n        # optional: gradient clipping (example)\n        # scaler.unscale_(optimizer)\n        # torch.nn.utils.clip_grad_norm_(model_cls.parameters(), max_norm=1.0)\n\n        scaler.step(optimizer)\n        scaler.update()\n\n        batch_size = imgs.size(0)\n        running_loss += loss.item() * batch_size\n        n_samples += batch_size\n\n        # collect predictions & labels on CPU (avoid storing GPU tensors)\n        with torch.no_grad():\n            probs = torch.sigmoid(outputs).detach().cpu().numpy()   # (B, C)\n            tlabels = labels.detach().cpu().numpy()\n        train_preds_cpu.append(probs)\n        train_targets_cpu.append(tlabels)\n\n        pbar.set_postfix({\"loss\": f\"{running_loss / n_samples:.4f}\"})\n\n    train_loss = running_loss / max(1, n_samples)\n    train_preds = np.vstack(train_preds_cpu)\n    train_targets = np.vstack(train_targets_cpu)\n\n    # threshold at 0.5 for binary predictions; can tune per-class thresholds later\n    train_pred_bin = (train_preds >= 0.5).astype(int)\n\n    # metrics: macro F1, micro F1, samples F1 (samples average better for multi-label)\n    train_f1_macro = f1_score(train_targets, train_pred_bin, average=\"macro\", zero_division=0)\n    train_f1_micro = f1_score(train_targets, train_pred_bin, average=\"micro\", zero_division=0)\n    train_f1_samples = f1_score(train_targets, train_pred_bin, average=\"samples\", zero_division=0)\n\n    # ---------------- VALIDATION ----------------\n    model_cls.eval()\n    val_running_loss = 0.0\n    val_n = 0\n    val_preds_cpu = []\n    val_targets_cpu = []\n\n    with torch.no_grad():\n        pbar = tqdm(val_loader, desc=f\"Val Epoch {epoch}\", leave=False)\n        for batch in pbar:\n            if len(batch) == 3:\n                imgs, labels, _ = batch\n            else:\n                imgs, labels = batch\n\n            imgs = imgs.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True).float()\n\n            with autocast(device_type=\"cuda\"):\n                outputs = model_cls(imgs)\n                loss = criterion(outputs, labels)\n\n            b = imgs.size(0)\n            val_running_loss += loss.item() * b\n            val_n += b\n\n            probs = torch.sigmoid(outputs).detach().cpu().numpy()\n            tlabels = labels.detach().cpu().numpy()\n            val_preds_cpu.append(probs)\n            val_targets_cpu.append(tlabels)\n\n            pbar.set_postfix({\"val_loss\": f\"{val_running_loss / val_n:.4f}\"})\n\n    val_loss = val_running_loss / max(1, val_n)\n    val_preds = np.vstack(val_preds_cpu)\n    val_targets = np.vstack(val_targets_cpu)\n    val_pred_bin = (val_preds >= 0.5).astype(int)\n\n    val_f1_macro = f1_score(val_targets, val_pred_bin, average=\"macro\", zero_division=0)\n    val_f1_micro = f1_score(val_targets, val_pred_bin, average=\"micro\", zero_division=0)\n    val_f1_samples = f1_score(val_targets, val_pred_bin, average=\"samples\", zero_division=0)\n\n    # optional: per-class F1\n    try:\n        per_class_f1 = f1_score(val_targets, val_pred_bin, average=None, zero_division=0)\n    except Exception:\n        per_class_f1 = None\n\n    # optional: per-class AUROC (requires at least one pos & neg per class)\n    try:\n        per_class_auroc = []\n        for c in range(NUM_CLASSES):\n            if len(np.unique(val_targets[:, c])) > 1:\n                per_class_auroc.append(roc_auc_score(val_targets[:, c], val_preds[:, c]))\n            else:\n                per_class_auroc.append(np.nan)\n        per_class_auroc = np.array(per_class_auroc)\n    except Exception:\n        per_class_auroc = None\n\n    # scheduler step (ReduceLROnPlateau expects validation metric)\n    scheduler.step(val_loss)\n\n    # Logging\n    print(\n        f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\\n\"\n        f\"  Train F1 (macro/micro/samples): {train_f1_macro:.4f}/{train_f1_micro:.4f}/{train_f1_samples:.4f}\\n\"\n        f\"  Val   F1 (macro/micro/samples): {val_f1_macro:.4f}/{val_f1_micro:.4f}/{val_f1_samples:.4f}\"\n    )\n    if per_class_f1 is not None:\n        print(\"  Per-class F1 (first 5):\", np.round(per_class_f1[:5], 3))\n\n    if per_class_auroc is not None:\n        print(\"  Per-class AUROC (first 5):\", np.round(per_class_auroc[:5], 3))\n\n    # save best model by val_loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model_cls.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"scaler_state\": scaler.state_dict(),\n            \"val_loss\": val_loss,\n        }, best_path)\n        print(\"✅ Saved best model ->\", best_path)\n\n# Save final weights\ntorch.save(model_cls.state_dict(), MODEL_DIR / \"resnet50_multilabel_final.pth\")\nprint(\"🏁 Done. Best val loss:\", best_val_loss)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:28:38.178650Z","iopub.execute_input":"2025-10-03T15:28:38.178976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Testing & Visualization for your ResNet50 multilabel model ===\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, classification_report\ntry:\n    from torchmetrics.classification import MultilabelAUROC\n    _HAS_TORCHMETRICS = True\nexcept Exception:\n    _HAS_TORCHMETRICS = False\n\n# grad-cam imports (pytorch-grad-cam)\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n\n# -----------------------\n# Helpers: load checkpoint\n# -----------------------\ndef load_model_from_checkpoint(model, ckpt_path, device):\n    ckpt = torch.load(ckpt_path, map_location=device)\n    # ckpt could be either state_dict or wrapped dict\n    if 'model_state' in ckpt:\n        state = ckpt['model_state']\n    else:\n        state = ckpt\n    model.load_state_dict(state)\n    model.to(device)\n    model.eval()\n    return model\n\n# -----------------------\n# Inference on a loader\n# -----------------------\n@torch.no_grad()\ndef infer_loader(model, loader, device):\n    model.eval()\n    all_probs = []\n    all_targets = []\n    for batch in tqdm(loader, desc=\"Infer\"):\n        # batch might contain id\n        if len(batch) == 3:\n            imgs, labels, _ = batch\n        else:\n            imgs, labels = batch\n        imgs = imgs.to(device, non_blocking=True)\n        labels = labels.cpu().numpy()\n        outputs = model(imgs)                  # logits\n        probs = torch.sigmoid(outputs).cpu().numpy()\n        all_probs.append(probs)\n        all_targets.append(labels)\n    all_probs = np.vstack(all_probs)\n    all_targets = np.vstack(all_targets)\n    return all_probs, all_targets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:58:40.467147Z","iopub.execute_input":"2025-10-06T14:58:40.467815Z","iopub.status.idle":"2025-10-06T14:58:46.902991Z","shell.execute_reply.started":"2025-10-06T14:58:40.467785Z","shell.execute_reply":"2025-10-06T14:58:46.902230Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Recreate val_loader for inference (no multiprocessing)\nval_loader = DataLoader(\n    val_ds,\n    batch_size=BATCH,\n    shuffle=False,\n    num_workers=0,    # <-- important for notebook stability\n    pin_memory=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:58:53.768451Z","iopub.execute_input":"2025-10-06T14:58:53.768879Z","iopub.status.idle":"2025-10-06T14:58:53.773009Z","shell.execute_reply.started":"2025-10-06T14:58:53.768858Z","shell.execute_reply":"2025-10-06T14:58:53.772452Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport torch\n\ndef multilabel_metrics(y_true, y_probs, threshold=0.5, verbose=True):\n    \"\"\"\n    y_true: numpy array shape (N, C) float or int (0/1)\n    y_probs: numpy array shape (N, C) of probabilities\n    Returns dict of metrics. Uses torchmetrics if available and inputs converted correctly.\n    \"\"\"\n    y_true_np = (y_true >= 0.5).astype(int)  # ensure 0/1 ints for sklearn\n    y_pred_np = (y_probs >= threshold).astype(int)\n\n    # sklearn metrics\n    from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n\n    f1_macro = f1_score(y_true_np, y_pred_np, average='macro', zero_division=0)\n    f1_micro = f1_score(y_true_np, y_pred_np, average='micro', zero_division=0)\n    f1_samples = f1_score(y_true_np, y_pred_np, average='samples', zero_division=0)\n    precision_macro = precision_score(y_true_np, y_pred_np, average='macro', zero_division=0)\n    recall_macro = recall_score(y_true_np, y_pred_np, average='macro', zero_division=0)\n\n    # per-class AUROC with sklearn (guard classes with single label)\n    per_class_auroc = []\n    for c in range(y_true_np.shape[1]):\n        if len(np.unique(y_true_np[:, c])) > 1:\n            per_class_auroc.append(roc_auc_score(y_true_np[:, c], y_probs[:, c]))\n        else:\n            per_class_auroc.append(np.nan)\n    per_class_auroc = np.array(per_class_auroc)\n\n    # torchmetrics multilabel AUROC (optional)\n    auroc_tm = None\n    if _HAS_TORCHMETRICS:\n        try:\n            import torch as _torch\n            # torchmetrics wants preds (float) and target (int/long)\n            preds_t = _torch.tensor(y_probs, dtype=_torch.float32, device='cpu')\n            target_t = _torch.tensor(y_true_np, dtype=_torch.long, device='cpu')  # IMPORTANT: long/int\n            m_auroc = MultilabelAUROC(num_labels=y_true_np.shape[1])\n            auroc_tm = m_auroc(preds_t, target_t).numpy()\n        except Exception as e:\n            # fallback silently to sklearn results\n            if verbose:\n                print(\"torchmetrics AUROC failed:\", e)\n            auroc_tm = None\n\n    if verbose:\n        print(\"F1 (macro/micro/samples):\", f1_macro, f1_micro, f1_samples)\n        print(\"Precision (macro):\", precision_macro, \"Recall (macro):\", recall_macro)\n        if auroc_tm is not None:\n            print(\"Torchmetrics Multilabel AUROC:\", auroc_tm)\n        print(\"Per-class AUROC (first 8):\", np.round(per_class_auroc[:8], 4))\n\n    return {\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"f1_samples\": f1_samples,\n        \"precision_macro\": precision_macro,\n        \"recall_macro\": recall_macro,\n        \"per_class_auroc\": per_class_auroc,\n        \"torchmetrics_auroc\": auroc_tm,\n        \"y_pred\": y_pred_np\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:58:55.999903Z","iopub.execute_input":"2025-10-06T14:58:56.000177Z","iopub.status.idle":"2025-10-06T14:58:56.011388Z","shell.execute_reply.started":"2025-10-06T14:58:56.000155Z","shell.execute_reply":"2025-10-06T14:58:56.010374Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# -----------------------\n# Grad-CAM visualization\n# -----------------------\n# For ResNet50, choose last conv layer: model.layer4[-1].conv3 (commonly used)\ndef get_resnet_target_layer(model):\n    try:\n        return model.layer4[-1].conv3\n    except Exception:\n        # fallback to last module with 4 dims\n        for m in reversed(list(model.modules())):\n            if isinstance(m, torch.nn.Conv2d):\n                return m\n    raise RuntimeError(\"Couldn't find conv target layer\")\n\ndef gradcam_on_image(model, img_path, target_class, transform_for_model, target_layer=None, device='cuda'):\n    \"\"\"\n    img_path: Path to .jpg\n    target_class: int class index to visualize (0..NUM_CLASSES-1)\n    transform_for_model: torchvision transform used for model input (e.g., val_tfms)\n    Returns: cam_image (RGB numpy array), overlay (RGB numpy array)\n    \"\"\"\n    # load and preprocess\n    img = Image.open(img_path).convert(\"RGB\")\n    # we need a normalized tensor for model, and an unnormalized float image for overlay\n    input_tensor = transform_for_model(img).unsqueeze(0)  # C,H,W -> 1,C,H,W\n    # build image for overlay: normalized to [0,1] with same H,W\n    img_for_overlay = np.array(img.resize((input_tensor.shape[-1], input_tensor.shape[-2]))) / 255.0\n    input_tensor = input_tensor.to(device)\n\n    # choose target layer\n    if target_layer is None:\n        target_layer = get_resnet_target_layer(model)\n\n    cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=(device == 'cuda'))\n    targets = [ClassifierOutputTarget(target_class)]\n    grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]   # H x W, values 0..1\n\n    # overlay\n    visualization = show_cam_on_image(img_for_overlay, grayscale_cam, use_rgb=True)\n    return visualization, grayscale_cam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:59:01.797634Z","iopub.execute_input":"2025-10-06T14:59:01.798507Z","iopub.status.idle":"2025-10-06T14:59:01.804869Z","shell.execute_reply.started":"2025-10-06T14:59:01.798478Z","shell.execute_reply":"2025-10-06T14:59:01.804188Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 1) Ensure val_loader recreated with num_workers=0 (run block A)\n# 2) Load checkpoint if not loaded already (reuse load_model_from_checkpoint)\nckpt_to_load = MODEL_DIR / \"resnet50_multilabel_best (1).pth\"\nif not ckpt_to_load.exists():\n    ckpt_to_load = MODEL_DIR / \"resnet50_multilabel_final.pth\"\nprint(\"Loading:\", ckpt_to_load)\nmodel = load_model_from_checkpoint(model_cls, ckpt_to_load, device)\n\n# 3) Infer\nprobs, targets = infer_loader(model, val_loader, device)  # returns numpy arrays\n\n# 4) Metrics\nmetrics = multilabel_metrics(targets, probs, threshold=0.5)\n\n\n# 3) Per-class report (top/bottom by AUROC)\nper_class_auroc = metrics[\"per_class_auroc\"]\nfor i, cname in enumerate(class_names):\n    print(f\"{i:02d} {cname:20s} AUROC: {per_class_auroc[i]:.4f}  (pos fraction: {targets[:,i].mean():.4f})\")\n\n# 4) Confusion-like summary: per-class F1 (using threshold 0.5)\ny_pred = metrics[\"y_pred\"]\nper_class_f1 = []\nfor c in range(NUM_CLASSES):\n    try:\n        per_class_f1.append(f1_score(targets[:,c], y_pred[:,c], zero_division=0))\n    except Exception:\n        per_class_f1.append(np.nan)\nprint(\"Per-class F1 (first 8):\", np.round(per_class_f1[:8], 4))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:59:05.811519Z","iopub.execute_input":"2025-10-06T14:59:05.811785Z","iopub.status.idle":"2025-10-06T14:59:17.607486Z","shell.execute_reply.started":"2025-10-06T14:59:05.811765Z","shell.execute_reply":"2025-10-06T14:59:17.606841Z"}},"outputs":[{"name":"stdout","text":"Loading: /kaggle/working/models/resnet50_multilabel_best (1).pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Infer:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62138335eae04421bf71699b01e6b938"}},"metadata":{}},{"name":"stdout","text":"F1 (macro/micro/samples): 0.4557527218433765 0.7765800042274361 0.8634333402980462\nPrecision (macro): 0.6049614860134169 Recall (macro): 0.40100188805834924\nTorchmetrics Multilabel AUROC: 0.94955\nPer-class AUROC (first 8): [     0.9816      0.9504      0.9133      0.9809      0.9784       0.941      0.9649      0.9398]\n00 Aortic_enlargement   AUROC: 0.9816  (pos fraction: 0.2060)\n01 Atelectasis          AUROC: 0.9504  (pos fraction: 0.0140)\n02 Calcification        AUROC: 0.9133  (pos fraction: 0.0267)\n03 Cardiomegaly         AUROC: 0.9809  (pos fraction: 0.1447)\n04 Consolidation        AUROC: 0.9784  (pos fraction: 0.0213)\n05 ILD                  AUROC: 0.9410  (pos fraction: 0.0280)\n06 Infiltration         AUROC: 0.9649  (pos fraction: 0.0467)\n07 Lung_Opacity         AUROC: 0.9398  (pos fraction: 0.0860)\n08 Nodule_Mass          AUROC: 0.9216  (pos fraction: 0.0567)\n09 Other_lesion         AUROC: 0.9059  (pos fraction: 0.0727)\n10 Pleural_effusion     AUROC: 0.9701  (pos fraction: 0.0693)\n11 Pleural_thickening   AUROC: 0.9453  (pos fraction: 0.1247)\n12 Pneumothorax         AUROC: 0.9091  (pos fraction: 0.0080)\n13 Pulmonary_fibrosis   AUROC: 0.9513  (pos fraction: 0.1140)\n14 No_finding           AUROC: 0.9896  (pos fraction: 0.7140)\nPer-class F1 (first 8): [     0.8351      0.1667      0.2333      0.8072      0.2667      0.4571      0.5128      0.3825]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# === Safe Inference + Grad-CAM overlays (robust) ===\nimport random, traceback\nfrom pathlib import Path\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# grad-cam imports\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# Ensure val_loader uses num_workers=0 for safety in notebook\nfrom torch.utils.data import DataLoader\nval_loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=0, pin_memory=True)\n\n# checkpoint selection (reuse)\nMODEL_DIR = Path(\"/kaggle/working/models\")\nbest_ckpt = MODEL_DIR / \"resnet50_multilabel_best (1).pth\"\nfinal_ckpt = MODEL_DIR / \"resnet50_multilabel_final.pth\"\nckpt_to_load = best_ckpt if best_ckpt.exists() else final_ckpt\nprint(\"Loading checkpoint:\", ckpt_to_load)\n\n# reload model architecture and weights\nmodel = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\nmodel.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\nmodel = load_model_from_checkpoint(model, ckpt_to_load, device)\n\n# ---------- Inference (already worked before) ----------\nprobs, targets = infer_loader(model, val_loader, device)\nmetrics = multilabel_metrics(targets, probs, threshold=0.5)\n\n# ---------- Grad-CAM helpers ----------\ndef get_resnet_target_layer(model):\n    # try most common: last conv of layer4\n    try:\n        return model.layer4[-1].conv3\n    except Exception:\n        # fallback: return last Conv2d\n        for m in reversed(list(model.modules())):\n            if isinstance(m, torch.nn.Conv2d):\n                return m\n    raise RuntimeError(\"No Conv2d layer found in model\")\n\ndef make_overlay_and_save(model, img_path: Path, class_idx: int, transform_for_model, save_path: Path, device_str: str):\n    \"\"\"\n    Creates Grad-CAM overlay for img_path and saves to save_path.\n    Returns True on success, False on failure.\n    \"\"\"\n    try:\n        # load raw PIL\n        pil = Image.open(img_path).convert(\"RGB\")\n        # create model input tensor (1,C,H,W)\n        inp_tensor = transform_for_model(pil).unsqueeze(0).to(device)\n\n        # create float image for overlay in range [0,1] with same H,W as model input\n        # transform_for_model may include normalization and ToTensor; we want raw resized image\n        # So resize pil to target HxW of the model input\n        # Determine H,W from inp_tensor shape\n        _, C, H, W = inp_tensor.shape\n        img_for_overlay = np.array(pil.resize((W, H))).astype(np.float32) / 255.0\n\n        # pick target layer\n        target_layer = get_resnet_target_layer(model)\n\n        # instantiate GradCAM (do not pass use_cuda kw - versions differ)\n        # GradCAM will automatically use CUDA if model is on CUDA\n        cam = GradCAM(model=model, target_layers=[target_layer])\n\n        targets = [ClassifierOutputTarget(class_idx)]\n        # compute cam (H x W)\n        grayscale_cam = cam(input_tensor=inp_tensor, targets=targets)[0]\n        # overlay\n        visualization = show_cam_on_image(img_for_overlay, grayscale_cam, use_rgb=True)\n\n        # save\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n        plt.imsave(save_path, visualization)\n        # try to cleanup internal hooks to avoid destructor warnings if available\n        if hasattr(cam, \"clear_hooks\"):\n            try:\n                cam.clear_hooks()\n            except Exception:\n                pass\n        return True\n    except Exception as e:\n        print(\"Grad-CAM failed for\", img_path, \"err:\", str(e))\n        traceback.print_exc()\n        return False\n\n# ---------- Generate Grad-CAMs for a few sample val images ----------\nout_dir = Path(\"gradcam_outputs\")\nout_dir.mkdir(exist_ok=True, parents=True)\n\nrandom.seed(42)\nsample_ids = random.sample(val_ids, min(12, len(val_ids)))\n\nprint(\"Creating Grad-CAMs for\", len(sample_ids), \"images ...\")\nfor img_id in sample_ids:\n    img_path = val_img_dir / f\"{img_id}.jpg\"\n    # compute probs for single image (fast)\n    with torch.no_grad():\n        img_tensor = val_tfms(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n        logits = model(img_tensor)\n        probs_img = torch.sigmoid(logits).cpu().numpy()[0]\n    # pick top predicted class\n    top_cls = int(np.argmax(probs_img))\n    save_path = out_dir / f\"{img_id}_top{top_cls}_{class_names[top_cls]}.png\"\n    ok = make_overlay_and_save(model, img_path, top_cls, val_tfms, save_path, device_str='cuda' if torch.cuda.is_available() else 'cpu')\n    if ok:\n        print(\"Saved:\", save_path, \" top_prob:\", probs_img[top_cls])\n    else:\n        print(\"Skipped:\", img_id)\n\nprint(\"Done. Grad-CAM outputs in:\", out_dir.resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T14:59:23.315894Z","iopub.execute_input":"2025-10-06T14:59:23.316474Z","iopub.status.idle":"2025-10-06T14:59:36.292362Z","shell.execute_reply.started":"2025-10-06T14:59:23.316449Z","shell.execute_reply":"2025-10-06T14:59:36.291563Z"}},"outputs":[{"name":"stdout","text":"Loading checkpoint: /kaggle/working/models/resnet50_multilabel_best (1).pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Infer:   0%|          | 0/47 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04cfd0bf419943b7837f10e1e22c3c5d"}},"metadata":{}},{"name":"stdout","text":"F1 (macro/micro/samples): 0.4557527218433765 0.7765800042274361 0.8634333402980462\nPrecision (macro): 0.6049614860134169 Recall (macro): 0.40100188805834924\nTorchmetrics Multilabel AUROC: 0.94955\nPer-class AUROC (first 8): [     0.9816      0.9504      0.9133      0.9809      0.9784       0.941      0.9649      0.9398]\nCreating Grad-CAMs for 12 images ...\nSaved: gradcam_outputs/a7ae31b4052444b8f576b657b1753445_top13_Pulmonary_fibrosis.png  top_prob: 0.8020872\nSaved: gradcam_outputs/f7d12f21dfedde3c815096d60fb5df44_top14_No_finding.png  top_prob: 0.9999883\nSaved: gradcam_outputs/01815f05a4a4e424181a6d3be101dcd0_top14_No_finding.png  top_prob: 0.9885522\nSaved: gradcam_outputs/902b03191f0a6bc2e7e47be16b63c38c_top0_Aortic_enlargement.png  top_prob: 0.99022794\nSaved: gradcam_outputs/a838e79ba2e9716bc790a76f7ae1c94e_top3_Cardiomegaly.png  top_prob: 0.84627056\nSaved: gradcam_outputs/c1253501ece20b237303679fb3b4310a_top14_No_finding.png  top_prob: 0.9999958\nSaved: gradcam_outputs/6c4e3ecdd85336bbdc77a16f7d8eb1bc_top14_No_finding.png  top_prob: 0.99235135\nSaved: gradcam_outputs/592041a42457197c22109a002d4b3348_top14_No_finding.png  top_prob: 0.9789531\nSaved: gradcam_outputs/b72e2e8714232b44cab432ed8530fd83_top14_No_finding.png  top_prob: 0.9999999\nSaved: gradcam_outputs/af15833e946fa12ce1815ffbb05a2f51_top14_No_finding.png  top_prob: 0.99988186\nSaved: gradcam_outputs/4a89b92ca6acb621d81b30707c95e8b8_top14_No_finding.png  top_prob: 0.99997807\nSaved: gradcam_outputs/06ba60d8ce256b3edecbb31323e37576_top14_No_finding.png  top_prob: 0.9994967\nDone. Grad-CAM outputs in: /kaggle/working/gradcam_outputs\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
